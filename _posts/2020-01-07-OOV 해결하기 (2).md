---
title: "OOV 해결하기 (1)-2"
excerpt: "Byte Pair Encoding 2부 - 실제 데이터 적용하기"
toc: true
toc_sticky: true

author_profile: false

date: 2020-01-07 16:00:00 -0000
categories: 
  - NLP
tags:
  - 
---

지난 게시글에서 BPE와 관련한 논문과 그 개념, 그리고 간단한 코드를 알아보았다.
>[BPE 개념](https://an-seunghwan.github.io/nlp/OOV-%ED%95%B4%EA%B2%B0%ED%95%98%EA%B8%B0-(1)/)
>
>[BPE 논문](https://an-seunghwan.github.io/nlp/Neural-Machine-Translation-of-Rare-Words-with-Subword-Units(%EB%85%BC%EB%AC%B8-%EC%9D%BD%EA%B8%B0)/)

이번 게시글에서는 실제 데이터에 적용하여 그 결과를 확인해보도록 하겠다. 

## 데이터 
* 네이버 영화 리뷰 데이터 'Naver sentiment movie corpus v1.0'
* 출처: [https://github.com/e9t/nsmc](https://github.com/e9t/nsmc)
* 본 글에서는 평가 데이터만을 예시로 사용한다.

## setup
```python
import pandas as pd
import re
from tqdm import tqdm
from konlpy.tag import Okt
from pprint import pprint

DATA_PATH = r'C:\Users\dpelt\Downloads\nsmc-master\nsmc-master'
```

## 파일 읽기
```python
data = pd.read_csv(DATA_PATH + '/' + FILE_NAME, 
                   header=0,
                   delimiter='\t',
                   quoting=3)
# 텍스트 데이터만을 따로 뽑아서 사용
text_data = data['document'][:1000]
text_data.head()
```
```
0                                                  굳 ㅋ
1                                 GDNTOPCLASSINTHECLUB
2               뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아
3                     지루하지는 않은데 완전 막장임... 돈주고 보기에는....
4    3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??
Name: document, dtype: object
```

## 한글화 정제
```python
def clean_korean(sent):
    if type(sent) == str:
        h = re.compile('[^가-힣ㄱ-ㅎㅏ-ㅣ\\s]+')
        result = h.sub(' ', sent)
    else:
        result = ''
    return result

clean_text = []
for i in tqdm(range(len(text_data))):
    sent = clean_korean(text_data.iloc[i])
    if len(sent) and sent != ' ': # 비어있는 데이터가 아닌지 확인
        clean_text.append(sent)
```
```
100%|██████████| 1000/1000 [00:00<00:00, 64009.77it/s]
```
```python
pprint(clean_text[:6])
```
```
['굳 ㅋ',
 '뭐야 이 평점들은  나쁘진 않지만  점 짜리는 더더욱 아니잖아',
 '지루하지는 않은데 완전 막장임  돈주고 보기에는 ',
 ' 만 아니었어도 별 다섯 개 줬을텐데  왜  로 나와서 제 심기를 불편하게 하죠 ',
 '음악이 주가 된  최고의 음악영화',
 '진정한 쓰레기']
```
 
## BPE 알고리즘

### 함수 정의
```python
def vocab_segmentation(vocab):
    '''
    전체 단어에 대해 segment하여 subwords로 구성된 단어 사전을 구축
    '''
    vocab_segment = set()
    for word in vocab.keys():
        symbols = word.split()
        vocab_segment.update(symbols)
    return list(vocab_segment)
    
def get_stats(vocab):
    '''
    각 byte pair의 빈도수를 계산
    '''
    pairs = collections.defaultdict(int) 
    for word, freq in vocab.items():
        symbols = word.split() 
        for i in range(len(symbols)-1):
            pairs[symbols[i], symbols[i+1]] += freq
    return pairs

def merge_vocab(pair, v_in):
    '''
    빈도수가 높은 byte pair로 기존의 vocab을 update
    '''
    v_out = {}
    bigram = re.escape(' '.join(pair))
    p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
    for word in v_in:
        w_out = p.sub(repl=''.join(pair), string=word)
        v_out[w_out] = v_in[word]
    return v_out
```

### step 1. 초기 단어 사전 구축
```python
vocab = init_vocab(clean_text)
subwords_dict = vocab_segmentation(vocab)
print('subwords 사전의 개수: {}'.format(len(subwords_dict)))
print(subwords_dict[:100])
```
```
subwords 사전의 개수: 991
['간', '베', '링', '적', '권', '딘', 'ㄹ', '펐', '룬', '뀐', '쒀', '잤', '이', '행', '탕', '듣', '맣', '해', '픽', '돈', '대', '젖', '흡', '숙', '똥', '쟎', '폭', '왜', '댄', '신', '렀', '낫', 'ㅗ', '눈', '빡', '처', '황', '굉', '학', '즌', '댓', '패', '윌', '근', '넼', '좔', '쟁', '주', '쭈', 'ㅜ', '매', '획', '끌', '런', '개', '놓', '갇', '욱', '관', '잃', '태', '앙', '렇', '란', '봉', '텐', '충', '졸', '끝', '툭', '풍', '냥', '피', '벤', 'ㅡ', '떄', '팡', '지', '컷', '다', '둥', '샤', '컨', '껄', '겉', '안', '흐', '뭔', '렌', '케', '쫙', '프', '빌', '봤', '짖', '과', '자', '직', '릭', '셸']
```
subwords 사전이 음절별로 초기에 구성되어 있고, 그 길이는 991이다.

### step 2. merge 과정과 단어 사전 update
<!--stackedit_data:
eyJoaXN0b3J5IjpbMjE0MjEwNTk5OCwtMTU1MTYxMjg4MywxMT
kwNDgyMzBdfQ==
-->