---
title: "Inference on Variable Importance Measure 2편"
excerpt: "pytorch로 구현하기!"
toc: true
toc_sticky: true

author_profile: false
use_math: true

date: 2021-12-02 20:00:00 -0000
categories: 
  - VIM
tags:
  - Inference
  - pytorch
---


> 참조논문
> 1. [Demystifying statistical learning based on efficient influence functions](https://arxiv.org/abs/2107.00681)
> 2. [Semiparametric doubly robust targeted double machine learning: a review](https://arxiv.org/abs/2203.06469)
> 3. [A General Framework for Inference on Algorithm-Agnostic Variable Importance](https://www.tandfonline.com/doi/full/10.1080/01621459.2021.2003200)
> 4. [Variable importance measures for heterogeneous causal effects](https://arxiv.org/pdf/2204.06030.pdf)

## 변수의 중요도에 대한 통계적 추론 (Inference on Variable Importance Measure (VIM))

```python
import os
os.chdir(os.path.dirname(os.path.abspath(__file__)))
import numpy as np
import pandas as pd
import tqdm
```

### Introduction

Simulation 데이터 세팅은 다음과 같다:

- 데이터 개수 $n = 1000$
- 변수 개수 $p = 5$
- treatment: $$A \sim Ber(\sigma(0.5 \cdot X_1 -0.4 \cdot X_2 + 0.3 \cdot X_3 + 0.2 \cdot X_4 -0.1 \cdot X_5))$$
	- $\sigma(\cdot)$은 sigmoid function
- Conditional ATE: $$\tau(X) = 1 \cdot X_1 -2 \cdot X_2 -3 \cdot X_3 -4 \cdot X_4 +5 \cdot X_5$$
- Outcome: $$Y \sim N(-5 \cdot X_1 -4 \cdot X_2 + 3 \cdot X_3 -2 \cdot X_4 +1 \cdot X_5 + A \cdot \tau(X), 1)$$

위의 확률 분포를 토대로, 아래와 같이 python으로 simulation data를 생성할 수 있다.

```python
np.random.seed(1)
n =  1000
p =  5
X =  np.random.uniform(low=-1,  high=1,  size=(n, p))
beta =  np.array([[0.5,  -0.4,  0.3,  0.2,  -0.1]])
logit = X @ beta.T
prob =  1  / (1  +  np.exp(-logit))
treatment =  np.random.binomial(n=1,  p=prob)

beta =  np.array([[1,  -2,  -3,  -4,  5]])
cate = X @ beta.T
beta =  np.array([[-5,  -4,  3,  -2,  1]])
outcome = X @ beta.T + treatment * cate +  np.random.normal(size=(n, 1))
  
data =  np.concatenate([X, treatment, outcome], axis=1)
covariates =  ['X{}'.format(i+1)  for i in  range(p)]
data =  pd.DataFrame(data, columns=covariates +  ['treatment',  'outcome'])
```

## Implementation

```python
from sklearn import linear_model
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import RandomForestClassifier
from sklearn import metrics

np.random.seed(0)
K =  2  # cross-fitted inference
m = data.shape[0]  // K
index_list =  [m * i for i in  range(K)]  +  [data.shape[0]]
```

- `K`는 cross-fitted inference를 위한 데이터 fold의 개수이고, `index_l

### Step 1. 관심있는 추정치를 정의


### Step 2. 추정치의 EIF를 계산


### Step 3. 추정치의 EIF를 이용하여 추정량을 계산


### Step 4. 통계적 추론


<!--stackedit_data:
eyJoaXN0b3J5IjpbLTg5NDQ3Mzc0OSwxODM3ODYwODIyLDIwNT
A5NzkxNTgsNjgwMzc1NjY4XX0=
-->