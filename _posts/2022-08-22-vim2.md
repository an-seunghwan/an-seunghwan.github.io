---
title: "Inference on Variable Importance Measure 2편"
excerpt: "pytorch로 구현하기!"
toc: true
toc_sticky: true

author_profile: false
use_math: true

date: 2021-12-02 20:00:00 -0000
categories: 
  - VIM
tags:
  - Inference
  - pytorch
---


> 참조논문
> 1. [Demystifying statistical learning based on efficient influence functions](https://arxiv.org/abs/2107.00681)
> 2. [Semiparametric doubly robust targeted double machine learning: a review](https://arxiv.org/abs/2203.06469)
> 3. [A General Framework for Inference on Algorithm-Agnostic Variable Importance](https://www.tandfonline.com/doi/full/10.1080/01621459.2021.2003200)
> 4. [Variable importance measures for heterogeneous causal effects](https://arxiv.org/pdf/2204.06030.pdf)

## 변수의 중요도에 대한 통계적 추론 (Inference on Variable Importance Measure (VIM))

### Introduction

- 데이터 개수 $n = 1000$
- 변수 개수 $p = 5$

```python
np.random.seed(1)
n =  1000
p =  5
X =  np.random.uniform(low=-1,  high=1,  size=(n, p))
beta =  np.array([[0.5,  -0.4,  0.3,  0.2,  -0.1]])
logit = X @ beta.T
prob =  1  / (1  +  np.exp(-logit))

treatment =  np.random.binomial(n=1,  p=prob)
beta =  np.array([[1,  -2,  -3,  -4,  5]])

cate = X @ beta.T
beta =  np.array([[-5,  -4,  3,  -2,  1]])
outcome = X @ beta.T + treatment * cate +  np.random.normal(size=(n, 1))
  
data =  np.concatenate([X, treatment, outcome], axis=1)
covariates =  ['X{}'.format(i+1)  for i in  range(p)]
data =  pd.DataFrame(data, columns=covariates +  ['treatment',  'outcome'])
```

## Implementation

```python
import os
os.chdir(os.path.dirname(os.path.abspath(__file__)))
import numpy as np
import pandas as pd
import tqdm
```

### Step 1. 관심있는 추정치를 정의


### Step 2. 추정치의 EIF를 계산


### Step 3. 추정치의 EIF를 이용하여 추정량을 계산


### Step 4. 통계적 추론


<!--stackedit_data:
eyJoaXN0b3J5IjpbLTExMzI5ODE4MDcsMTgzNzg2MDgyMiwyMD
UwOTc5MTU4LDY4MDM3NTY2OF19
-->