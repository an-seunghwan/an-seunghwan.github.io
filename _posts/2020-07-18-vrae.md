---
title: "Generating Sentences from a Continuous Space 1편(작성중)"
excerpt: "VAE + RNN을 알아보자"
toc: true
toc_sticky: true

author_profile: false
use_math: true

date: 2020-07-18 20:00:00 -0000
categories: 
  - VAE
  - NLP
tags:
  - tensorflow
  - keras
  - RNN
---

> [Generating Sentences from a Continuous Space](https://arxiv.org/abs/1511.06349) 논문에 대한 간단한 리뷰와 tensorflow 코드입니다. 
>  본 포스팅은 위의 내용에 대한 1편 입니다.
>  정확한 내용과 수식들은 논문을 참조해주시기 바랍니다. 
>  또한 아래 내용을 읽어보시다가 잘 와닿지 않으신다면 2편 코드 작성 내용을 읽어보시면 도움이 될 것입니다!

## 1. 구조

<center><img  src="https://github.com/an-seunghwan/an-seunghwan.github.io/blob/master/assets/img/vrae.png?raw=true" width="800"  height="200"></center>

논문에서 제안하는 모형은 생각보다 매우 간단하다. 기존의 sequence to sequence의 모형에 latent space를 도입한 것이 전부이다. 즉, LSTM layer로 구성된 encoder가 문장을 입력받으면, hidden state를 출력한다. 이를 이용해서 posterior 분포의 평균과 분산을 linear layer를 이용하여 계산하고, 이를 이용해 latent variable $z$를 생성한다. 다음 $z$를 LSTM layer로 구성된 decoder에 decoder 입력 문장과 함께 input으로 넣어준다. 

(다음 2편 포스팅에서 코드를 살펴본다면 구조가 더 명확히 보일 것이다!)

## 2. 최적화 방안들

### 1. KL annealing

ELBO 식에서 KL-divergence term에 weight를 추가하여 training을 진행한다. 학습 초기에는 이 weight를 0으로 설정하고, 학습이 점점 진행(iteration 진행)할수록 1까지 증가시킨다. 

이러한 방법을 적용하면 training의 초기에는 KL-divergence를 학습에 반영하지 않고 reconstruction의 좋은 성능을 위해 $z$에 encoder로부터 얻은 정보를 최대한 반영할 수 있도록 학습을 진행한다.

학습이 진행되다가 마지막에는 weight이 1에 가까워지고, 이때 실제 ELBO의 objective의 값과 동일하게 된다.

이 weight은 hyperparameter로써 조절되며, logistic function이나 linear function을 이용해 조절된다.

### 2. word dropout and historyless decoding

decoder가 너무 powerful해져서 $z$를 사용하지 않고도 reconstruction을 충분히 잘 수행할 수 있으므로, decoder를 약화(weaken)시킨다. 이때 사용하는 방법은 $z$에 대한 decoder의 의존도를 높이기 위해, decoder에 input으로 들어가는 단어들에 dropout을 수행한다.

dropout

## 3. 활용

### 1. 기존과의 큰 차이점

### 2. imputing missing words

### 3. latent interpolation


<!--stackedit_data:
eyJoaXN0b3J5IjpbMTU0Nzc4MDQ5NCwtMTc2NDcxMDY3MywzNz
Q0MTI0OTddfQ==
-->