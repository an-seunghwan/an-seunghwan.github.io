---
title: "Generating Sentences from a Continuous Space 1편(작성중)"
excerpt: "VAE + RNN을 알아보자"
toc: true
toc_sticky: true

author_profile: false
use_math: true

date: 2020-07-18 20:00:00 -0000
categories: 
  - VAE
  - NLP
tags:
  - tensorflow
  - keras
  - RNN
---

> [Generating Sentences from a Continuous Space](https://arxiv.org/abs/1511.06349) 논문에 대한 간단한 리뷰와 tensorflow 코드입니다. 
>  본 포스팅은 위의 내용에 대한 1편 입니다.
>  정확한 내용과 수식들은 논문을 참조해주시기 바랍니다. 
>  또한 아래 내용을 읽어보시다가 잘 와닿지 않으신다면 2편 코드 작성 내용을 읽어보시면 도움이 될 것입니다!

## 1. 구조

<center><img  src="https://github.com/an-seunghwan/an-seunghwan.github.io/blob/master/assets/img/vrae.png?raw=true" width="800"  height="200"></center>

논문에서 제안하는 모형은 생각보다 매우 간단하다. 기존의 sequence to sequence의 모형에 latent space를 도입한 것이 전부이다. 즉, LSTM layer로 구성된 encoder가 문장을 입력받으면, hidden state를 출력한다. 이를 이용해서 posterior 분포의 평균과 분산을 linear layer를 이용하여 계산하고, 이를 이용해 latent variable $z$를 생성한다. 다음 $z$를 LSTM layer로 구성된 decoder에 decoder 입력 문장과 함께 input으로 넣어준다. 

(다음 2편 포스팅에서 코드를 살펴본다면 구조가 더 명확히 보일 것이다!)

## 2. 최적화 방안들

### 1. KL annealing

ELBO 식에서 KL-divergence term에 weight를 추가하여 training을 진행한다. 학습 초기에는 이 weight를 0으로 설정하고, 학습이 점점 진행(iteration 진행)할수록 1까지 증가시킨다. 

이러한 방법을 적용하면 training의 초기에는 KL-divergence를 학습에 반영하지 않고 reconstruction의 좋은 성능을 위해 $z$에 encoder로부터 얻은 정보를 최대한 반영할 수 있도록 학습을 진행한다.

학습이 진행되다가 마지막에는 weight이 1에 가까워지고, 이때 실제 ELBO의 objective의 값과 동일하게 된다.

이 weight은 hyperparameter로써 조절되며, logistic function이나 linear function을 이용해 조절된다.

### 2. word dropout and historyless decoding

decoder가 너무 powerful해져서 $z$를 사용하지 않고도 reconstruction을 충분히 잘 수행할 수 있으므로, decoder를 약화(weaken)시킨다. 이때 사용하는 방법은 $z$에 대한 decoder의 의존도를 높이기 위해, decoder에 input으로 들어가는 단어들에 dropout을 수행한다.

dropout에 해당되는 단어들은 `<UNK>`라는 특수 단어로 변환되어 입력에 사용된다. 극단적으로 dropout ratio가 1인 경우, 모든 decoder의 input이 `<UNK>`으로 변환되어 decoder는 $z$만을 사용하게되어 의존도가 매우 높아진다.

## 3. 활용

### 1. 기존과의 큰 차이점

(논문을 읽고 느낀 내용)

기존의 NLP 논문들을 조금 몇 편 읽어보면서 가장 크게 생각한 것은, 제안하는 알고리즘의 성능을 평가할 수 있는 metric이 조금 부족하다는 것이었다. 따라서 수치적 제시대신 해당 모형이 보여주는 좋은 결과 예시 등을 논문에서 보여줌으로써 이를 대신하는 경우가 있다고 생각한다.

하지만, VAE를 기반으로한 이 NLP 모형은 ELBO라는 아주 좋은 성능 평가 지표가 있다. 또한 Gaussian 분포를 사용하기 때문에 closed form으로 계산이 가능하다.

### 2. imputing missing words

decoder에 들어가는 input 단어들 중, 빈칸이 있을 때 beam search를 이용해 data log-likelihood가 커지도록 채워넣는 성능이 매우 높다(이 결과는 나에게 흥미로운 주제는 아니었다).

### 3. latent interpolation

이 3번의 결과가 본 논문의 모형에 대해 공부하고자 한 가장 큰 요소였다. 이 latent interpolation은 서로 다른 문장 2개가 있을 

<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE1MzYzMjY0NDcsLTE3NjQ3MTA2NzMsMz
c0NDEyNDk3XX0=
-->