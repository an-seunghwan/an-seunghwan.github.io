---
title: "Generating Sentences from a Continuous Space 1편(작성중)"
excerpt: "VAE + RNN을 알아보자"
toc: true
toc_sticky: true

author_profile: false
use_math: true

date: 2020-07-18 20:00:00 -0000
categories: 
  - VAE
  - NLP
tags:
  - tensorflow
  - keras
  - RNN
---

> [Generating Sentences from a Continuous Space](https://arxiv.org/abs/1511.06349) 논문에 대한 간단한 리뷰와 tensorflow 코드입니다. 
>  본 포스팅은 위의 내용에 대한 1편 입니다.
>  정확한 내용과 수식들은 논문을 참조해주시기 바랍니다. 

## 1. 구조

<center><img  src="https://github.com/an-seunghwan/an-seunghwan.github.io/blob/master/assets/img/vrae.png?raw=true" width="800"  height="200"></center>

논문에서 제안하는 모형은 생각보다 매우 간단하다. 기존의 sequence to sequence의 모형에 latent space를 도입한 것이 전부이다. 즉, LSTM layer로 구성된 encoder가 문장을 입력받으면, hidden state를 출력한다. 이를 이용해서 posterior 분포의 평균과 분산을 linear layer를 이용하여 계산하고, 이를 이용해 latent variable $z$를 생성한다. 다음 $z$를 LSTM layer로 구성된 decoder에 decoder 입력 문장과 함께 input으로 넣어준다. 

## 2. 최적화 방안들

### 1. KL annealing

### 2. word dropout and historyless decoding

## 3. 활용

### 1. 기존과의 큰 차이점

### 2. imputing missing words

### 3. latent interpolation


<!--stackedit_data:
eyJoaXN0b3J5IjpbMTkzMDg4MzAzNV19
-->