# 전처리

## 1. tokenizing
* `.`, `,`, `/`, `&` 등의 특수문자 등을 모두 제외하면 안된다.
	- binary classifier를 이용해 `.`의 단어의 일부분(약어로 쓰이는 경우)인 경우와 문장의 구분자인 경우를 구분
	- **binary classifier는 일정한 규칙 또는 머신러닝으로 학습한 모형일 수 있다.**
	- 다른 tokenizing 여부가 쉽게 결정되지 않는 경우들에 대해 적용?
* 단어 내에 띄어쓰기가 있거나, `-`으로 연결되어 있는 경우, 이를 하나의 단어로 인식해야 한다.
* **문장 tokenizing**: how to...?

## 2. 품사 태깅(Part-of-speech tagging)
1. 정확한 의미 파악

단어의 표기가 같지만 그 품사에 따라 단어의 의미가 달라질 수 있다.
`ex) 명사 '안': 내부에서 라는 의미 vs 부사 '안': 부정의 의미`
따라서 tokenizing 과정에서 단어의 품사를 구분해야 정확한 의미를 파악할 수 있다.

2. 유의미한 단어의 품사만을 활용하면 모형의 성능이 향상될 수 있다.
	* 명사, 동사, 형용사 등
	* 의미있는 품사 단어만을 사용 vs 전체 단어 사용 **(성능 비교)**

## 3. 정제와 정규화
tokenizing 전, 후에 이루어지는 과정
* **정제(cleaning)**: 갖고 있는 corpus로부터 noise 데이터를 제거
* **정규화(normalization)**: 의미는 같지만 표현이 다른 단어들을 하나의 단어로 통합시킨다.
	- 목적: 단어의 개수가 많아질수록, 문서 전체에 대한 단어 사전의 크기가 매우 커진다. → 다뤄야 하는 데이터의 크기가 매우 커진다! (**단어의 개수를 최소로 하는 것은 매우 중요하다**)

### 1. 규칙에 기반한 표기가 다른 단어들의 통합
1. 어간 추출(stemming)
2. 표제어 추출(lemmatization)

### 2. 불필요한 단어를 제거 
1. 등장 빈도가 적은 단어
2. 길이가 지나치게 짧은 단어 
→ 한글에서는 일반적으로 길이가 1인 단어

> 출처: [https://wikidocs.net/book/2155](https://wikidocs.net/book/2155)


<!--stackedit_data:
eyJoaXN0b3J5IjpbMzA2NTUzODZdfQ==
-->