---
title: "Keras로 이해하는 seq2seq (1) (작성중)"
excerpt: "Introduction & RNN(Recurrent Neural Network)"
toc: true
toc_sticky: true

author_profile: false
use_math: true

date: 2020-01-22 17:00:00 -0000
categories: 
  - NLP
tags:
  - tensorflow 2.0
  - keras
---
> 이 포스팅은[Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)의 리뷰와 동시에 RNN, LSTM, GRU, teacher forcing 등의 추가적인 내용들을 다루고 있습니다!

## Why RNN?

기존의 DNN(deep neural network)은 고정된 길이의 input과 output에 대해서만 적용이 가능하다는 문제를 가지고 있다. 따라서, 미리 output 데이터의 길이를 알지 못하는 경우(기계번역, 질의응답 알고리즘)에는 DNN을 사용할 수 없다. 

이러한 길이가 제한되지 않는 유연한 sequence 형태의 데이터를 다루기 위한 방법론으로 sequence to sequence mapping 방법론이 제시되었다. 

RNN, 순환 신경망은 기존의 잘 알려진 DNN과 유사하지만, 은닉층(hidden layer)의 결과 벡터가 다시 은닉층에 입력된다는 점이 가장 다르다. 이러한 점 때문에 재귀적(recurrent)이라는 이름이 붙게 되었다. 또한 기존에 은닉층과 같이 사용되는 노드(node)라는 용어 대신 **cell**이라는 용어를 더 많이 사용한다. 즉, RNN hidden layer의 cell이 DNN hidden layer의 node에 대응된다고 생각하면 좋다.



용어를 다음과 같이 정리하자.

$$
t: \text{시점}
$$

## 참고자료
- [https://stackoverflow.com/questions/38714959/understanding-keras-lstms](https://stackoverflow.com/questions/38714959/understanding-keras-lstms)
- SUTSKEVER, Ilya; VINYALS, Oriol; LE, Quoc V. Sequence to sequence learning with neural networks. In: _Advances in neural information processing systems_. 2014. p. 3104-3112.
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE4MjMyNTExNTEsLTIwMDk3NDk5NjAsMj
ExNjIwNDc1MF19
-->