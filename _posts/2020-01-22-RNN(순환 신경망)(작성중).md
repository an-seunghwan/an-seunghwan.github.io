---
title: "Keras로 이해하는 RNN(순환 신경망)(작성중)"
excerpt: "Recurrent Neural Network"
toc: true
toc_sticky: true

author_profile: false
use_math: true

date: 2020-01-22 17:00:00 -0000
categories: 
  - NLP
tags:
  - tensorflow 2.0
  - keras
---
> 이 글

## Why RNN?

기존의 DNN(deep neural network)은 고정된 길이의 input과 output에 대해서만 적용이 가능하다는 문제를 가지고 있다.

RNN, 순환 신경망은 기존의 잘 알려진 DNN과 유사하지만, 은닉층(hidden layer)의 결과 벡터가 다시 은닉층에 입력된다는 점이 가장 다르다. 이러한 점 때문에 재귀적(recurrent)이라는 이름이 붙게 되었다. 또한 기존에 은닉층과 같이 사용되는 노드(node)라는 용어 대신 **cell**이라는 용어를 더 많이 사용한다. 즉, RNN hidden layer의 cell이 DNN hidden layer의 node에 대응된다고 생각하면 좋다.



용어를 다음과 같이 정리하자.

$$
t: \text{시점}
$$

## 참고자료
- [https://stackoverflow.com/questions/38714959/understanding-keras-lstms](https://stackoverflow.com/questions/38714959/understanding-keras-lstms)
- SUTSKEVER, Ilya; VINYALS, Oriol; LE, Quoc V. Sequence to sequence learning with neural networks. In: _Advances in neural information processing systems_. 2014. p. 3104-3112.
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTM1NDcyNzgzNSwtMjAwOTc0OTk2MCwyMT
E2MjA0NzUwXX0=
-->