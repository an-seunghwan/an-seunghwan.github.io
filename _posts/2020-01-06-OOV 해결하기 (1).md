---
title: "OOV 해결하기 (1)"
excerpt: "Byte Pair Encoding"
toc: true
toc_sticky: true

author_profile: false

date: 2020-01-06 22:00:00 -0000
categories: 
  - NLP
tags:
  - 
---
## What is OOV?

OOV = '욕심꾸러기'

Byte = 한 음절

## 예제

## setup
```python
import re, collections
from pprint import pprint
```

### 단어 사전

corpus에서 주어진 `단어 : 빈도수` 사전이 다음과 같다고 가정하자.
```python
vocab = {'장 난 꾸 러 기 </w>': 5,
         '잠 꾸 러 기 </w>': 6,
         '장 난 감 </w>': 10,
         '잠 수 </w>': 3,
         '욕 심 </w>': 4}
```
**띄어쓰기로 각 음절을 구분해 놓은 것에 주목해야 한다.** 즉, 처음으로 단어 사전을 구성할 때는 각 음절이 별도로 취급되어야 한다.

초기 단어 사전의 형태는 다음과 같다(이를 단어 사전의 segmentation이라 하자).
```python
def dict_segmentation(vocab):
    initial_vocab = set()
    for word in vocab.keys():
        symbols = word.split()
        initial_vocab.update(symbols[:-1])
    pprint(initial_vocab)

dict_segmentation(vocab)
```
```python
{'장', '욕', '감', '난', '수', '심', '러', '잠', '꾸', '기'}
```

### Byte 조합의 빈도수 계산 함수

```python
def get_stats(vocab):
    pairs = collections.defaultdict(int) # 값을 저장할 빈 dict
    for word, freq in vocab.items():
        symbols = word.split() # 띄어쓰기를 기준으로 분할
        for i in range(len(symbols)-1):
            pairs[symbols[i], symbols[i+1]] += freq
    return pairs
```
초기 단어 사전에 대해 어떤 결과가 나오는 지 살펴보자.
```python
pprint(get_stats(vocab))
```
```
defaultdict(<class 'int'>,
            {('감', '</w>'): 10,
             ('기', '</w>'): 11,
             ('꾸', '러'): 11,
             ('난', '감'): 10,
             ('난', '꾸'): 5,
             ('러', '기'): 11,
             ('수', '</w>'): 3,
             ('심', '</w>'): 4,
             ('욕', '심'): 4,
             ('잠', '꾸'): 6,
             ('잠', '수'): 3,
             ('장', '난'): 15})
```

### 빈도수를 기준으로 Byte를 Pairing!

가장 높은 빈도수의 Byte 쌍을 추출한다.
```python
pairs = get_stats(vocab)
best = max(pairs, key=pairs.get)
print(best)
```
```
('장', '난')
```
`('장', '난')` byte 쌍이 빈도수가 15로 가장 높다. 이제 다음의 함수를 이용해 앞의 byte를 pairing한다.

> merge_vocab의 detail한 해석!
```python
def merge_vocab(pair, v_in):
    v_out = {}
    bigram = re.escape(' '.join(pair))
    p = re.compile(r'(?<!\S)' + bigram + r'(?!\S)')
    for word in v_in:
        w_out = p.sub(''.join(pair), word)
        v_out[w_out] = v_in[word]
    return v_out
```
```python
vocab = merge_vocab(best, vocab)
pprint(vocab)
```
```
{'욕 심 </w>': 4,
 '잠 꾸 러 기 </w>': 6,
 '잠 수 </w>': 3,
 '장난 감 </w>': 10,
 '장난 꾸 러 기 </w>': 5}
```
가장 빈도수가 높았던 byte 쌍인 `('장', '난')` 가 하나의 음절로 묶인 것을 볼 수 있다. 이제 어떠한 단어 사전의 segmentation을 확인하자.
```python
dict_segmentation(vocab)
```
```
{'장난', '욕', '감', '수', '심', '러', '잠', '꾸', '기'}
```

> BPE 알고리즘 논문 : https://arxiv.org/pdf/1508.07909.pdf  
> BPE 알고리즘 논문 저자의 깃허브 : https://github.com/rsennrich/subword-nmt
<!--stackedit_data:
eyJoaXN0b3J5IjpbMTMxMTAwNTE2MSw0MTk4OTk5MTcsNTgxND
M4NzQsNTkyMjIzNDgwLDE1ODI2NjMyNzZdfQ==
-->