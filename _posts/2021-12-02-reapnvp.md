---
title: "DENSITY ESTIMATION USING REAL NVP"
excerpt: "Real NVP를 tensorflow로 구현하기"
toc: true
toc_sticky: true

author_profile: false
use_math: true

date: 2021-12-02 20:00:00 -0000
categories: 
  - tensorflow 2.0
tags:
  - tensorflow 2.0
  - keras
---

> Keras 공식 홈페이지의 [https://keras.io/examples/generative/real_nvp/](https://keras.io/examples/generative/real_nvp/)의 코드를 기반으로 작성되었습니다.
> Real NVP 논문: [DENSITY ESTIMATION USING REAL NVP](https://arxiv.org/pdf/1605.08803.pdf)

## Real NVP

Normalizing Flow를 이용한 분포 추정에서, 널리 사용되는 방법 중 하나인 Real NVP를 tensorflow와 keras로 구현해보겠다. 해당 논문에 관한 이론적인 내용은 기회가 된다면 추후 포스팅 할 예정!

### 0. import 

```python
import tensorflow as tf
import tensorflow.keras as K
from tensorflow.keras import layers
from tensorflow.keras.utils import to_categorical
print('TensorFlow version:', tf.__version__)
print('Eager Execution Mode:', tf.executing_eagerly())
print('available GPU:', tf.config.list_physical_devices('GPU'))
from tensorflow.python.client import device_lib
print('==========================================')
print(device_lib.list_local_devices())
```

```
TensorFlow version: 2.4.0 
Eager Execution Mode: True 
available GPU: [] 
========================================== 
[name: "/device:CPU:0" 
device_type: "CPU" 
memory_limit: 268435456 
locality { } 
incarnation: 10055035958512198800 
]
```

### 1. dataset

dataset은 scikit-learn에서 제공하는 moons dataset을 이용하였다.

```python
from sklearn.datasets import make_moons
import numpy as np
import matplotlib.pyplot as plt

data = make_moons(3000, noise=0.05)[0].astype("float32")
data.shape
```

```
(3000, 2)
```

### 2. 모형 정의와 학습에 필요한 parameter 정의

```python
PARAMS = {
    'data_dim': 2,
    'embedding_dim': 256,
    'reg': 0.01,
    'coupling_MLP_num': 4,
    'K': 8,
    'latent_dim': 2,
    'nf_dim': 1,
}
```

### 3. Coupling layer 

coupling layer는 `relu` activation을 갖는 hidden layer로 구성된 MLP이다. hidden layer의 개수나 units의 수는 적합하고자 하는 데이터의 분포에 맞게 조절하면 된다!

```python
#%%
class CouplingLayer(K.models.Model):
    def __init__(self, params, embedding_dim, output_dim, activation, name='CouplingLayer', **kwargs):
        super(CouplingLayer, self).__init__(name=name, **kwargs)
        
        self.params = params
        self.embedding_dim = embedding_dim
        self.output_dim = output_dim
        self.activation = activation
        self.dense = [
            layers.Dense(self.embedding_dim, activation='relu', kernel_regularizer=K.regularizers.l2(self.params['reg'])) 
            for _ in range(self.params['coupling_MLP_num'])
            ] + [
            layers.Dense(self.output_dim, activation=self.activation, kernel_regularizer=K.regularizers.l2(self.params['reg']))
            ]
        
    def call(self, x):
        for d in self.dense:
            x = d(x)
        return x
```

### 4. Normalizing Flow



```python
#%%
class NormalizingFlow(K.models.Model):
    def __init__(self, params, name='NormalizingFlow', **kwargs):
        super(NormalizingFlow, self).__init__(name=name, **kwargs)
        
        self.params = params
        self.mask = np.array(
            [[0, 1], [1, 0]] * (self.params['K'] // 2), dtype="float32"
        )
        self.s = [CouplingLayer(self.params, self.params['embedding_dim'], self.params['nf_dim'], activation='tanh')
                    for _ in range(self.params['K'])]
        self.t = [CouplingLayer(self.params, self.params['embedding_dim'], self.params['nf_dim'], activation='linear')
                    for _ in range(self.params['K'])]
        
        self.loss_tracker = K.metrics.Mean(name="loss")
        
    @property
    def metrics(self):
        """List of the model's metrics.
        We make sure the loss tracker is listed as part of `model.metrics`
        so that `fit()` and `evaluate()` are able to `reset()` the loss tracker
        at the start of each epoch and at the start of an `evaluate()` call.
        """
        return [self.loss_tracker]
        
    def call(self, x, sum_log_abs_det_jacobians=None):
        if sum_log_abs_det_jacobians is None:
            sum_log_abs_det_jacobians = 0
        log_abs_det_jacobian = 0
        
        for i in range(self.params['K']):
            x_masked = tf.boolean_mask(x, self.mask[i], axis=1)
            x = (
                x * self.mask[i]
                + (1 - self.mask[i])
                * (x * tf.repeat(tf.math.exp(self.s[i](x_masked)), 2, axis=1) 
                   + 
                   tf.repeat(self.t[i](x_masked), 2, axis=1))
            )
            
            log_abs_det_jacobian += tf.reduce_sum(self.s[i](x_masked), axis=-1)
        sum_log_abs_det_jacobians += log_abs_det_jacobian
        return x, sum_log_abs_det_jacobians
    
    def inverse(self, x):
        for i in reversed(range(self.params['K'])):
            x_masked = tf.boolean_mask(x, self.mask[i], axis=1)
            x = (
                x * self.mask[i]
                + (1 - self.mask[i])
                * ((x - tf.repeat(self.t[i](x_masked), 2, axis=1))
                   *
                   tf.repeat(tf.math.exp(- self.s[i](x_masked)), 2, axis=1))
            )
        return x
    
    def build_graph(self):
        dummy = tf.random.normal((1, self.params['data_dim']))
        _ = self(dummy)
        return print('Graph is built!')
    
    def log_likelihood(self, x):
        x, sum_log_abs_det_jacobians = self(x)
        loss = tf.reduce_mean(tf.reduce_sum(tf.square(x), axis=1) / 2 - sum_log_abs_det_jacobians)
        return loss
    
    def train_step(self, data):
        with tf.GradientTape() as tape:
            loss = self.log_likelihood(data)

        grad = tape.gradient(loss, self.trainable_variables)
        self.optimizer.apply_gradients(zip(grad, self.trainable_variables))
        self.loss_tracker.update_state(loss)
        return {"loss": self.loss_tracker.result()}    
    
    def test_step(self, data):
        loss = self.log_likelihood(data)
        self.loss_tracker.update_state(loss)
        return {"loss": self.loss_tracker.result()}
```

### Reference
- Dinh, L., Sohl-Dickstein, J., & Bengio, S. (2016). Density estimation using real nvp. _arXiv preprint arXiv:1605.08803_.
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTIxNDM3MTE2NDddfQ==
-->