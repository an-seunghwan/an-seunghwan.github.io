---
title: "seq2seq 톺아보기 (1) (작성중)"
excerpt: "Introduction & RNN(Recurrent Neural Network)"
toc: true
toc_sticky: true

author_profile: false
use_math: true

date: 2020-01-22 17:00:00 -0000
categories: 
  - NLP
tags:
  - tensorflow 2.0
  - keras
  - rnn
---
> 톺아보기 시리즈
> 
> 이 포스팅은[Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)의 리뷰인 동시에 RNN, LSTM, GRU, teacher forcing 등의 추가적인 내용들을 다루는 *seq2seq 모형 톺아보기* 시리즈의 1편입니다.

## Introduction: Why seq2seq?

기존의 DNN(deep neural network)은 고정된 길이의 input과 output에 대해서만 적용이 가능하다는 문제를 가지고 있다. 따라서, 미리 output 데이터의 길이를 알지 못하는 경우(기계번역, 질의응답 알고리즘)에는 DNN을 사용할 수 없다. 

길이가 제한되지 않는 유연한 sequence 형태의 데이터를 다루기 위한 방법론으로 sequence to sequence mapping 방법론이 제시되었다. 이는 흔히 **seq2seq** 모형으로 불리며 이를 구성하는 방법과 모형의 형태는 매우 다양하고, 문제의 영역에 크게 구애받지 않는다는(domain-independent method) 장점이 있다. 현재 seq2seq 모형은 sequence 형식의 데이터를 다루는 가장 대표적인 모형이 되었고, 본 포스팅 시리즈는 **기계번역** 분야에 초점을 맞춰서 진행을 하려고 한다.

## RNN 

### <span style="color:#2E86C1;">0. 배경</span>

RNN이 등장한 배경은 사람이 생각하는 방식을 따라하기 위해 만들었다고 생각하면 이해하기 쉽다. 사람은 모든 것을 처음부터 새로 생각하지 않는다. 우리는 이전에 받아들인 정보를 버리거나 하지 않고 이를 바탕으로 새로운 것을 생각하게 된다. 즉, 사람의 사고는 지속성이 있다. 하지만, 기존의 신경망 모형은 이러한 문제를 해결하기가 어려웠다. 

RNN은 이러한 문제를 **loop**를 가지는 신경망 모형을 사용하여 해결하였고, 이러한 loop는 정보가 신경망 연산을 진행하는 동안에 지속될 수 있도록 해준다. 어떠한 방식으로 작동하는지 자세히 살펴보자.

이 포스팅에서 자주 사용되는 notation은 다음과 같다.

$$
\mathbf{x}: \text{sequence input data} \\
\mathbf{h}: \text{hidden state} \\
t: \text{timestep}
$$

### 1. 구조

[사진]

위 사진은 RNN  모형의 일부분을 나타내는 것이다. 이때 **cell**은 RNN layer를 구성하는 요소로써 DNN hidden layer의 node에 대응되는 역할이라고 생각하면 된다. input으로 $\mathbf{x}$의 $t$번째 timestep에 해당하는 $\mathbf{x}_t$를 받아 $\mathbf{h}_t$를 output으로 낸다. 하지만 잘 보면 화살표가 다시 cell로 들어가는 것을 볼 수 있다. 이는 앞에서 언급한 loop에 해당하는 부분으로 cell의 output인 $\mathbf{h}_t$가 다시 input으로써 cell에 입력되고, 이러한 점 때문에 재귀적(recurrent)이라는 이름이 붙게 되었다.

따라서 RNN의 loop을 풀어서 표현한다면 다음과 같이 나타낼 수 있다. 

[사진]

즉, 동일한 cell의 연속(복사본)으로 RNN layer는 구성되어 있는 것이다. 위의 chain과 같은 신경망 모형의 구성은 자연스럽게 sequence나 list를 다루는데 적절하도록 만들어준다.

### 2. 수식

아래 그림은 $t$ 시점의 하나의 cell의 구성을 나타낸다.

[그림]

- $\textbf{x}_t$
t timestep input of sequence, shape = ($d_{\mathbf{x}}$, 1)

- $\mathbf{h}_{t}$
hidden state corresponding to $t$th timestep input, shape = ($d_{\mathbf{h}}$, 1)

- $\mathbf{W}_\mathbf{x}$
input sequence에 대한 가중치 행렬, shape = ($d_{\mathbf{h}}$, $d_{\mathbf{x}}$) 

- $\mathbf{W}_\mathbf{h}$
hidden state에 대한 가중치 행렬, shape = ($d_{\mathbf{h}}$, $d_{\mathbf{h}}$) 

- $\mathbf{W}_\mathbf{y}$
output에 대한 가중치 행렬

일반적으로 $\mathbf{x}_t$는 기계번역 분야에서 하나의 단어(token)으로 간주된다. 따라서 $\mathbf{x}_t$의 차원인 $d_{\mathbf{x}}$는 embedding 차원의 크기(embedding size)라고 할 수 있다.

(입력과 출력의 길이가 달라 다양한 활용 가능)

### 3. 문제점

RNN의 loop(chain)과 같은 형태는 이전의 정보를 현재의 문제와 연결해준다는 아주 매력적인 점이 있다. 따라서 현재의 과제를 처리하기 위해 최근의 정보만이 필요한 경우, RNN은 매우 유용하다. 예를 들면, "점심을 굶어서 ___"라는 문장이 주어졌을 때, 꽤 명백하게 "배고파"라는 단어가 들어가면 적절할 것으로 예상된다.

하지만, 최근의 정보가 아닌 다른 문맥이 필요한 경우가 있다. 예를 들면, "나는 통계를 전공하고 현재 4학년 학부생... 그래서 나는 ___ 과목에 제일 자신있어."라는 문장이 주어진 경우에는, 예측하려는 단어("통계")와 관련된 문맥인 "나는 통계를 전공하고"는 그 거리(gap)이 매우 크다. 이렇게 기계번역에서 예측하려는 단어와 그 단어와 관련한 맥락 단어들의 거리가 먼 경우를 "**long term dependencies**"라고 말한다. 이론적으로는 RNN 모형이 long term dependencies를 해결할 수 있다고 하지만, 실제로는 RNN 모형을 이용해서는 이러한 문제를 해결하기가 매우 어렵다. 

### 4. with Keras

## 참고자료
- [https://stackoverflow.com/questions/38714959/understanding-keras-lstms](https://stackoverflow.com/questions/38714959/understanding-keras-lstms)
- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [The Unreasonable Effectiveness of Recurrent Neural Networks](https://karpathy.github.io/2015/05/21/rnn-effectiveness/)
- SUTSKEVER, Ilya; VINYALS, Oriol; LE, Quoc V. Sequence to sequence learning with neural networks. In: _Advances in neural information processing systems_. 2014. p. 3104-3112.
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTEyODcwNjkwNzYsNjAxNDAxMDY2LDEzMz
U5MDY5NDgsLTEyNzk0Mzk3NDEsLTE5Mjc2MzcxNTIsLTEwMzA0
ODM1NzRdfQ==
-->