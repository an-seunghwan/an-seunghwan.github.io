---
title: "seq2seq 톺아보기 (1) (작성중)"
excerpt: "Introduction & RNN(Recurrent Neural Network)"
toc: true
toc_sticky: true

author_profile: false
use_math: true

date: 2020-01-22 17:00:00 -0000
categories: 
  - NLP
tags:
  - tensorflow 2.0
  - keras
---
> 이 포스팅은[Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)의 리뷰와 [colah's: Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)의 번역인 동시에 RNN, LSTM, GRU, teacher forcing 등의 추가적인 내용들을 다루는 *seq2seq 모형 톺아보기* 시리즈의 1편입니다.

## Introduction: Why seq2seq?

기존의 DNN(deep neural network)은 고정된 길이의 input과 output에 대해서만 적용이 가능하다는 문제를 가지고 있다. 따라서, 미리 output 데이터의 길이를 알지 못하는 경우(기계번역, 질의응답 알고리즘)에는 DNN을 사용할 수 없다. 

길이가 제한되지 않는 유연한 sequence 형태의 데이터를 다루기 위한 방법론으로 sequence to sequence mapping 방법론이 제시되었다. 이는 흔히 **seq2seq** 모형으로 불리며 이를 구성하는 방법과 모형의 형태는 매우 다양하고, 문제의 영역에 크게 구애받지 않는다는(domain-independent method) 장점이 있다. 현재 seq2seq 모형은 sequence 형식의 데이터를 다루는 가장 대표적인 모형이 되었고, 본 포스팅 시리즈는 **기계번역** 분야에 초점을 맞춰서 진행을 하려고 한다.

## RNN 

### 0. 배경

RNN이 등장한 배경은 사람이 생각하는 방식을 따라하기 위해 만들었다고 생각하면 이해하기 쉽다. 사람은 모든 것을 처음부터 새로 생각하지 않는다. 우리는 이전에 받아들인 정보를 버리거나 하지 않고 이를 바탕으로 새로운 것을 생각하게 된다. 즉, 사람의 사고는 지속성이 있다. 하지만, 기존의 신경망 모형은 이러한 문제를 해결하기가 어려웠다. 

RNN은 이러한 문제를 **loop**를 가지는 신경망 모형을 사용하여 해결하였고, 이러한 loop는 정보가 신경망 연산을 진행하는 동안에 지속될 수 있도록 해준다. 어떠한 방식으로 작동하는지 자세히 살펴보자.

이 포스팅에서 자주 사용되는 notation은 다음과 같다.

$$
\textbf{x}, \textbf{h}: \text{sequence format data} \\
t: \text{timestep}
$$

### 1. 구조

[사진]

위 사진은 RNN  모형의 일부분을 나타내는 것이다. 이때 **cell**은 RNN layer를 구성하는 요소로써 DNN hidden layer의 node에 대응되는 역할이라고 생각하면 된다. input으로 $\textbf{x}$의 $t$번째 timestep에 해당하는 $\textbf{x}_t$를 받아 $\textbf{h}_t$를 output으로 낸다. 하지만 잘 보면 화살표가 다시 cell로 들어가는 것을 볼 수 있다. 이는 앞에서 언급한 loop에 해당하는 부분으로 cell의 output인 $\textbf{h}_t$가 다시 input으로써 cell에 입력되고, 이러한 점 때문에 재귀적(recurrent)이라는 이름이 붙게 되었다.

따라서 RNN의 loop을 풀어서 표현한다면 다음과 같이 나타낼 수 있다. 

[사진]

즉, 동일한 cell의 연속(복사본)으로 RNN layer는 구성되어 있는 것이다. 위의 chain과 같은 신경망 모형의 구성은 자연스럽게 sequence나 list를 다루는데 모형이 


## 참고자료
- [https://stackoverflow.com/questions/38714959/understanding-keras-lstms](https://stackoverflow.com/questions/38714959/understanding-keras-lstms)
- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- SUTSKEVER, Ilya; VINYALS, Oriol; LE, Quoc V. Sequence to sequence learning with neural networks. In: _Advances in neural information processing systems_. 2014. p. 3104-3112.
<!--stackedit_data:
eyJoaXN0b3J5IjpbNTQ0NzU4NDcwLC0xOTI3NjM3MTUyLC0xMD
MwNDgzNTc0XX0=
-->