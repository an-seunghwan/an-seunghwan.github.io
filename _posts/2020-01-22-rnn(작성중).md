---
title: "seq2seq 톺아보기 (1) (작성중)"
excerpt: "Introduction & RNN(Recurrent Neural Network)"
toc: true
toc_sticky: true

author_profile: false
use_math: true

date: 2020-01-22 17:00:00 -0000
categories: 
  - NLP
tags:
  - tensorflow 2.0
  - keras
---
> 이 포스팅은[Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)의 리뷰와 동시에 RNN, LSTM, GRU, teacher forcing 등의 추가적인 내용들을 다루는 *seq2seq 모형 톺아보기* 시리즈의 1편입니다.

## Introduction: Why seq2seq?

기존의 DNN(deep neural network)은 고정된 길이의 input과 output에 대해서만 적용이 가능하다는 문제를 가지고 있다. 따라서, 미리 output 데이터의 길이를 알지 못하는 경우(기계번역, 질의응답 알고리즘)에는 DNN을 사용할 수 없다. 

이러한 길이가 제한되지 않는 유연한 sequence 형태의 데이터를 다루기 위한 방법론으로 sequence to sequence mapping 방법론이 제시되었다. 이는 흔히 **seq2seq** 모형으로 불리며 이를 구성하는 방법과 모형의 형태는 매우 다양하고, 문제의 영역에 크게 구애받지 않는다는(domain-independent method) 장점이 있다.

현재 seq2seq 모형은 sequence 형식의 데이터를 다루는 가장 대표적인 모형이 되었고, 본 포스팅 시리즈는 **기계번역** 분야에 초점을 맞춰서 진행을 하려고 한다.

## RNN

RNN, 순환 신경망은 기존의 잘 알려진 DNN과 유사하지만, 은닉층(hidden layer)의 결과 벡터가 다시 은닉층에 입력된다는 점이 가장 다르다. 이러한 점 때문에 재귀적(recurrent)이라는 이름이 붙게 되었다. 또한 기존에 은닉층과 같이 사용되는 노드(node)라는 용어 대신 **cell**이라는 용어를 더 많이 사용한다. 즉, RNN hidden layer의 cell이 DNN hidden layer의 node에 대응된다고 생각하면 좋다.



용어를 다음과 같이 정리하자.

$$
t: \text{시점}
$$

## 참고자료
- [https://stackoverflow.com/questions/38714959/understanding-keras-lstms](https://stackoverflow.com/questions/38714959/understanding-keras-lstms)
- SUTSKEVER, Ilya; VINYALS, Oriol; LE, Quoc V. Sequence to sequence learning with neural networks. In: _Advances in neural information processing systems_. 2014. p. 3104-3112.
<!--stackedit_data:
eyJoaXN0b3J5IjpbNDA1Njg1MzEyXX0=
-->