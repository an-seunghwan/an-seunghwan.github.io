---
title: "seq2seq 톺아보기 (1) (작성중)"
excerpt: "Introduction & RNN(Recurrent Neural Network)"
toc: true
toc_sticky: true

author_profile: false
use_math: true

date: 2020-01-22 17:00:00 -0000
categories: 
  - NLP
tags:
  - tensorflow 2.0
  - keras
---
> 이 포스팅은[Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)의 리뷰와 [colah's: Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)의 번역인 동시에 RNN, LSTM, GRU, teacher forcing 등의 추가적인 내용들을 다루는 *seq2seq 모형 톺아보기* 시리즈의 1편입니다.

## Introduction: Why seq2seq?

기존의 DNN(deep neural network)은 고정된 길이의 input과 output에 대해서만 적용이 가능하다는 문제를 가지고 있다. 따라서, 미리 output 데이터의 길이를 알지 못하는 경우(기계번역, 질의응답 알고리즘)에는 DNN을 사용할 수 없다. 

길이가 제한되지 않는 유연한 sequence 형태의 데이터를 다루기 위한 방법론으로 sequence to sequence mapping 방법론이 제시되었다. 이는 흔히 **seq2seq** 모형으로 불리며 이를 구성하는 방법과 모형의 형태는 매우 다양하고, 문제의 영역에 크게 구애받지 않는다는(domain-independent method) 장점이 있다. 현재 seq2seq 모형은 sequence 형식의 데이터를 다루는 가장 대표적인 모형이 되었고, 본 포스팅 시리즈는 **기계번역** 분야에 초점을 맞춰서 진행을 하려고 한다.

## RNN 

### 0. 배경

RNN이 등장한 배경은 사람이 생각하는 방식을 따라하기 위해 만들었다고 생각하면 이해하기 쉽다. 사람은 모든 것을 처음부터 새로 생각하지 않는다. 우리는 이전에 받아들인 정보를 버리거나 하지 않고 이를 바탕으로 새로운 것을 생각하게 된다. 즉, 사람의 사고는 지속성이 있다. 하지만, 기존의 신경망 모형은 이러한 문제를 해결하기가 어려웠다. 

RNN은 이러한 문제를 **loop**를 가지는 신경망 모형을 사용하여 해결하였고, 이러한 loop는 정보가 신경망 연산을 진행하는 동안에 지속될 수 있도록 해준다. 어떠한 방식으로 작동하는지 자세히 살펴보자.

이 포스팅에서 자주 사용되는 notation은 다음과 같다.

$$
\textbf{x}, \textbf{h}: \text{sequence format data} \\
t: \text{timestep}
$$

### 1. 구조

[사진]

위 사진은 RNN  모형의 일부분을 나타내는 것이다. 이때 **cell**은 RNN layer를 구성하는 요소로써 DNN hidden layer의 node에 대응되는 역할이라고 생각하면 된다. input으로 $\textbf{x}$의 $t$번째 timestep에 해당하는 $\textbf{x}_t$를 받아 $\textbf{h}_t$를 output으로 낸다. 하지만 잘 보면 

우선 첫 번째로, 가장 기본적인 seq2seq의 구성 요소인 RNN에 대해 알아보자. RNN(순환 신경망)은 기존의 잘 알려진 DNN과 유사하지만, 은닉층(hidden layer)의 결과 벡터가 다시 은닉층에 입력된다는 점이 가장 다르다. 이러한 점 때문에 재귀적(recurrent)이라는 이름이 붙게 되었다. 또한 기존에 은닉층과 같이 사용되는 노드(node)라는 용어 대신 **cell**이라는 용어를 더 많이 사용한다. 즉, RNN hidden layer의 cell이 DNN hidden layer의 node에 대응된다고 생각하면 좋다.

### 1. Notation
용어를 다음과 같이 정리하자.



이때, $\textbf{x}_t$는 $t$ 시점의 input을 의미하며 마찬가지로 $\textbf{y}_t$는 $t$ 시점의 output을 의미한다.

### 2. mechanism

RNN은 input data $\textbf{x}$를 output data $\textbf{y}$로 mapping하는 hidden layer으로, 특히 시점 $t$에 대응되도록 다음과 같이 mapping을 하게 된다. 

$$
\textbf{x}_t \rightarrow \text{RNN cell} \rightarrow \textbf{y}_t
$$

그렇다면 어떠한 이유로 *recurrent*라는 이름이 붙게 되었을까? 위의 수식은 사실 생략된 부분이 많다. 더 자세히 풀어쓰면 다음과 같다.

$$
\dots \rightarrow \textbf{x}_t \rightarrow \text{RNN cell $t$} \rightarrow \textbf{h}_t \rightarrow \text{RNN cell $t+1$} \rightarrow
$$



## 참고자료
- [https://stackoverflow.com/questions/38714959/understanding-keras-lstms](https://stackoverflow.com/questions/38714959/understanding-keras-lstms)
- [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- SUTSKEVER, Ilya; VINYALS, Oriol; LE, Quoc V. Sequence to sequence learning with neural networks. In: _Advances in neural information processing systems_. 2014. p. 3104-3112.
<!--stackedit_data:
eyJoaXN0b3J5IjpbNzI3NzI2NDgyLC0xOTI3NjM3MTUyLC0xMD
MwNDgzNTc0XX0=
-->