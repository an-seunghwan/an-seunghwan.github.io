---
title: "ALMOND 논문 리뷰"
excerpt: "Adaptive Latent Modeling and Optimization via Neural Networks and Langevin Diffusion"
toc: true
toc_sticky: true

author_profile: false
use_math: true

date: 2021-09-05 20:00:00 -0000
categories: 
  - VAE
tags:
  - 논문 읽기
---


- [ALMOND: Adaptive Latent Modeling and Optimization via Neural Networks and Langevin Diffusion](https://www.tandfonline.com/doi/full/10.1080/01621459.2019.1691563) 논문에 대한 리뷰에 대한 간단한 제 생각을 적은 포스팅입니다.
- 자세하고 정확한 내용은 논문을 참고해 주세요!

## ALMOND

### generative model 

$$
\begin{aligned} 
x|u &\sim f_{\theta}(x|u) \\
p(x) &= \int_u f_{\theta}(x|u) \pi(u) du 
\end{aligned}
$$

이때, $\pi(u)$은 implicit distribution으로 우리가 알 수 없는 분포이다.

### assumption

$u \in \mathbb{R}^r$이라고 하면, $u$는 저차원의 manifold에 존재하므로, $d < r$에 대해서 다음과 같이 가정할 수 있다.

$$
\begin{aligned} 
z &\sim \pi_0(z) \\
u &= h_{\eta}(z)
\end{aligned}
$$

이때, $\pi_0(z)$는 고정된 쉬운 분포, 예를 들어 $N(0, I)$과  같은 분포를 생각한다. 따라서, 우리는 다음과 같은 $x$의 conditional distribution을 생각한다.

$$x|z \sim f_{\theta}(x|h_{\eta}(z))$$

### ELBO

목적함수 ELBO는 다음과 같이 쓰여질 수 있다.

$$
\begin{aligned} 
\log f_{\beta}(x) &= \log \int f_{\beta}(x|h_{\eta}(z)) \pi_0(z) dz \\
&\geq \int  \log \Bigg( \frac{f_{\beta}(x|h_{\eta}(z)) \pi_0(z)}{p_{\tilde{\beta}}(z|x)} \Bigg) p_{\tilde{\beta}}(z|x) dz \cdots (*)
\end{aligned}
$$

두번째 부등호는 Jensen's inequality에 의해 성립하며, 주의할 점은 $p_{\tilde{\beta}}(z|x)$가 $\beta$에 무관한 latent conditional distribution이라는 점이다. ($\beta = (\theta, \eta)$)

$\beta$의 update를 위해 $(*)$식을 미분하면 다음과 같다.

$$
\begin{aligned} 
\frac{\partial}{\partial \beta} (*) &= \int  \Bigg( \frac{\frac{\partial}{\partial \beta} f_{\beta}(x|h_{\eta}(z))}{f_{\beta}(x|h_{\eta}(z))} \Bigg) p_{\tilde{\beta}}(z|x) dz \\
&= \int  \log \Big( \frac{\partial}{\partial \beta} f_{\beta}(x|h_{\eta}(z)) \Big) p_{\tilde{\beta}}(z|x) dz \\
\end{aligned}
$$

$\beta$에 대한 미분을 위해서 적분이 필요하지만 위 식의 적분은 intractable하며, 대신 Monte Carlo approximation을 위해서는 $p_{\tilde{\beta}}(z|x)$으로부터의 sampling이 필요하다. 

만약 우리가 $t$시점에서 $t+1$ 시점으로의 $\beta$에 대한 update를 생각한다면, 고정된 $\tilde{\beta}$를 $\beta_{t}$으로 놓을 수 있다. 따라서 베이즈 정리에 의해

$$f_{\beta_t}(x|h_{\eta_t}(z)) \pi_0(z) \propto p_{\tilde{\beta}}(z|x)$$

와 같은 관계를 생각할 수 있으므로, 이 논문에서는 Langevin dynamic algorithm을 이용하여 $f_{\beta_t}(x|h_{\eta_t}(z)) \pi_0(z)$으로부터 $z$를 sampling하고, $\beta$에 대한 gradient를 estimate하여 ELBO에 대한 최대화를 수행한다. 추가적으로, approximate된 gradient를 사용한 경우의 결과에 대한 수렴성과 수렴속도에 대한 증명을 한 논문이다.

## Comments

- Conventional VAE에서 가정하는 posterior 분포대신

## Reference 
- Qiu, Y., & Wang, X. (2019). Almond: adaptive latent modeling and optimization via neural networks and Langevin diffusion. _Journal of the American Statistical Association_, 1-13.
<!--stackedit_data:
eyJoaXN0b3J5IjpbLTE5MDkzMTcwNDUsLTE4MDk2NzM1MTcsLT
g3MDQxMDE3MV19
-->