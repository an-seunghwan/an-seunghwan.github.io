---
title: "About NLP - 전처리"
excerpt: "전처리의 workflow"
toc: true
toc_sticky: true

author_profile: false

date: 2020-01-05 20:00:00 -0000
categories: 
  - NLP
tags:
  - 
---
## 데이터 
* 네이버 영화 리뷰 데이터 'Naver sentiment movie corpus v1.0'
* 출처: [https://github.com/e9t/nsmc](https://github.com/e9t/nsmc)
* 본 글에서는 평가 데이터만을 예시로 사용한다.

## setup
```python
import pandas as pd
import re
import pickle
from tqdm import tqdm
from konlpy.tag import Okt
```
## 파일 크기 확인
```python
import os

def get_file_size(file_name):
    size = round(os.path.getsize(DATA_PATH + '/ratings_test.txt') / 1000000, 2)
    print('file size: {} MB'.format(size))

FILE_NAME = 'ratings_test.txt'
get_file_size(FILE_NAME)
```
```
file size: 4.89 MB
```
## 파일 읽기
```python
data = pd.read_csv(DATA_PATH + '/' + FILE_NAME, 
                   header=0,
                   delimiter='\t',
                   quoting=3)
# 텍스트 데이터만을 따로 뽑아서 사용
text_data = data['document']
text_data.head()
```
```
0                                                  굳 ㅋ
1                                 GDNTOPCLASSINTHECLUB
2               뭐야 이 평점들은.... 나쁘진 않지만 10점 짜리는 더더욱 아니잖아
3                     지루하지는 않은데 완전 막장임... 돈주고 보기에는....
4    3D만 아니었어도 별 다섯 개 줬을텐데.. 왜 3D로 나와서 제 심기를 불편하게 하죠??
Name: document, dtype: object
```
만약 텍스트 데이터가 html 형식으로 구성되어 있다면, `BeautifulSoup`을 이용하여 html 문법에 해당하는 문자들을 모두 제거하고 본격적인 전처리에 돌입하는 것이 좋다.

* `BeautifulSoup`에 대한 사용법은 다른 게시글에서 자세히 다루겠습니다!

## 한글화 → 품사 태깅을 통한 정제와 정규화

### 1. 한글화 

일반적으로 한글의 텍스트 마이닝을 할 때, 영어와 같은 경우에는 단어 사전에 포함시켜야 하는 유의미한 단어인 경우가 매우 드물다. 즉, 특수한 전문 용어 이외에는 텍스트 분류나 유사도와 같은 분석 결과에 영향을 거의 미치지 않는다(숫자와 특수문자는 당연히 제외되어야 한다).

특히나, 예를 들면 감성 분석과 같은 텍스트 마이닝 과제를 통해 한글 감성 사전을 구축한다고 가정해보자. 이러한 경우에 'USA', '15.5%', 그리고 '10.2 points'와 같은 단어들은 감성이 담겨있지 않은 단어들이다. 따라서 이러한 단어들은 데이터에서 제외시켜야 할 필요가 있다.

다음은 문장에서 한글만을 추출하는 함수이다. 만약 입력된 `sent`가 문자열이 아니라면 빈 문자열을 반환하게 된다.
```python
def clean_korean(sent):
    if type(sent) == str:
        h = re.compile('[^가-힣ㄱ-ㅎㅏ-ㅣ\\s]+')
        result = h.sub('', sent)
    else:
        result = ''
    return result
```
```python
clean_korean(text_data.iloc[2])
```
```
'뭐야 이 평점들은 나쁘진 않지만 점 짜리는 더더욱 아니잖아'
```
원래의 데이터와 비교해보면 `.`과 10점의 `10`이 제거된 것을 볼 수 있다.

모든 데이터에 대해 위의 함수를 적용한다.

```python
clean_text = []
for i in tqdm(range(len(text_data))):
    sent = clean_korean(text_data.iloc[i])
    if len(sent) > 0: # 비어있는 데이터가 아닌지 확인
        clean_text.append(sent)
```
```
100%|██████████| 50000/50000 [00:00<00:00, 67248.44it/s]
```
한글화된 결과를 확인하자.
```python
pprint(clean_text[:6])
```
```
['굳 ㅋ',
 '뭐야 이 평점들은 나쁘진 않지만 점 짜리는 더더욱 아니잖아',
 '지루하지는 않은데 완전 막장임 돈주고 보기에는',
 '만 아니었어도 별 다섯 개 줬을텐데 왜 로 나와서 제 심기를 불편하게 하죠',
 '음악이 주가 된 최고의 음악영화',
 '진정한 쓰레기']
```

### 2. 품사 태깅을 통한 정제와 정규화

KoNLPy 라이브러리의 Okt를 이용하여 품사 태깅을 한다.

<!--stackedit_data:
eyJoaXN0b3J5IjpbMTIxMzYwMTMwMCw3MDY2MjgyNzYsLTI2Mj
k4Mzc1OV19
-->