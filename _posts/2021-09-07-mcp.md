---
title: "가중치에 MCP penalty가 적용된 신경망 모형을 구현해보자!"
excerpt: "custom kernel regularizer를 구현하기"
toc: true
toc_sticky: true

author_profile: false
use_math: true

date: 2021-09-07 20:00:00 -0000
categories: 
  - tensorflow 2.0
tags:
  - tensorflow 2.0
  - keras
  - custom modeling
---

## MCP penalty
Minimax Concave Penalty

$$
\begin{aligned} 
P_r(x;\lambda) = \begin{cases} \lambda \vert x \vert - \frac{x^2}{2r} \text{\quad if } \vert x \vert \leq r \lambda \\
\frac{r \lambda^2}{2} \text{\quad if } \vert x \vert > r \lambda \end{cases}
\end{aligned}
$$

이를 파이썬 함수로 구현하면 다음과 같다.

```python
@tf.function
def MCP(weight, lambda_, r):
	penalty1 = lambda_ * tf.abs(weight) - tf.math.square(weight) / (2. * r)
	penalty2 = tf.math.square(lambda_) * r / 2
	return  tf.reduce_sum(penalty1 * tf.cast(tf.abs(weight) <= r * lambda_, tf.float32) + penalty2 * tf.cast(tf.abs(weight) > r * lambda_, tf.float32))
```

MCP 함수의 개형을 다음의 코드로 같은 $\lambda$값을 가지는 Lasso penalty 함수와 비교해 볼 수 있다.

```python
plt.figure(figsize=(6, 4))
plt.plot([MCP(tf.cast(x, tf.float32), 3., 1.)  for x in np.linspace(-3, 3, 1000)], label='MCP')
plt.plot(3. * np.abs(np.linspace(-3, 3, 1000)), label='lasso')
plt.legend()
```

<center><img  src="https://github.com/an-seunghwan/an-seunghwan.github.io/blob/master/assets/img/mcp.png?raw=true" width="450"  height="300"></center>

## Regression with MCP 

구현한 MCP penalty가 변수선택을 제대로 수행해주는지 확인하기 위해 간단한 회귀분석을 수행한다.

### custom MCP regression layer
단순 선형 회귀분석을 하는 custom layer `MCPLayer`를 정의하고, `add_loss`를 이용해 MCP를 loss에 추가해준다.
```python
class MCPLayer(K.layers.Layer):
	def __init__(self, h, output_dim, lambda_, r, **kwargs):
		super(MCPLayer, self).__init__(**kwargs)
		self.input_dim = h.shape[-1]
		self.output_dim = output_dim
		self.lambda_ = lambda_
		self.r = r
		self.MCP = MCP
		w_init = tf.random_normal_initializer()
		self.w = tf.Variable(initial_value=w_init(shape=(self.input_dim, 1), 
							dtype='float32'), 
							trainable=True)

	def call(self, x):
		h = tf.matmul(x, self.w)
		self.add_loss(self.MCP(self.w, self.lambda_, self.r)) # loss에 MCP 추가
		return h
```

### experiment setting

```python
output_dim = 1
p = 100
n = 50
lambda_ = 5.
r =  2.
  
input_layer =  layers.Input((p))
custom_layer =  MCPLayer(input_layer, output_dim, lambda_, r)
outputs =  custom_layer(input_layer)
  
model =  K.models.Model(input_layer, outputs)
model.summary()
```

```
Model: "model" 
_________________________________________________________________ 
Layer (type) 				Output Shape 		Param # 
================================================================= 
input_1 (InputLayer) 		[(None, 100)] 		0 
_________________________________________________________________ 
mcp_layer (MCPLayer) 		(None, 1) 			100 
================================================================= 
Total params: 100 
Trainable params: 100 
Non-trainable params: 0 
_________________________________________________________________
```

```
beta = np.zeros((p, 1))
beta[:4, 0] = np.array([1, 2, 3, -4]) # 처음 4개의 회귀계수만 0이 아니다!
X = np.random.normal(size=(n, p))
y = X @ beta + np.random.normal(size=(n, 1))
```

### training

```python
optimizer = K.optimizers.SGD(0.0007)

for i in range(10000):
	with tf.GradientTape() as tape:
		yhat = model(X)
		loss = tf.reduce_sum(tf.math.square(yhat - y))
		loss += model.losses # MCP penalty loss 추가
	grad = tape.gradient(loss, model.trainable_weights)
	optimizer.apply_gradients(zip(grad, model.trainable_weights))
 
	if i % 100:
		diff = tf.reduce_sum(tf.math.square(model.weights[0] - beta)) # 실제 값과의 차이를 이용해 stopping rule 정의
		print(diff)
		if diff < 1:
			break	
```

```
tf.Tensor(24.235529, shape=(), dtype=float32) 
tf.Tensor(22.534918, shape=(), dtype=float32) 
tf.Tensor(21.274326, shape=(), dtype=float32) 
tf.Tensor(20.300333, shape=(), dtype=float32) 
tf.Tensor(19.521292, shape=(), dtype=float32) 
tf.Tensor(18.88015, shape=(), dtype=float32) 
tf.Tensor(18.340311, shape=(), dtype=float32) 
tf.Tensor(17.877214, shape=(), dtype=float32) 
tf.Tensor(17.473879, shape=(), dtype=float32) 
tf.Tensor(17.118221, shape=(), dtype=float32) 
tf.Tensor(16.801067, shape=(), dtype=float32) 
tf.Tensor(16.515059, shape=(), dtype=float32) 
tf.Tensor(16.254675, shape=(), dtype=float32) 
tf.Tensor(16.01548, shape=(), dtype=float32) 
tf.Tensor(15.794283, shape=(), dtype=float32) 
tf.Tensor(15.58833, shape=(), dtype=float32) 
tf.Tensor(15.395755, shape=(), dtype=float32) 
tf.Tensor(15.214806, shape=(), dtype=float32) 
tf.Tensor(15.044276, shape=(), dtype=float32) 
tf.Tensor(14.882601, shape=(), dtype=float32) 
tf.Tensor(14.7288685, shape=(), dtype=float32) 
tf.Tensor(14.582401, shape=(), dtype=float32) 
tf.Tensor(14.442043, shape=(), dtype=float32) 
tf.Tensor(14.307769, shape=(), dtype=float32) 
tf.Tensor(14.178376, shape=(), dtype=float32)

show more (open the raw output data in a text editor) ...

tf.Tensor(1.007096, shape=(), dtype=float32) 
tf.Tensor(1.004746, shape=(), dtype=float32) 
tf.Tensor(1.003016, shape=(), dtype=float32) 
tf.Tensor(1.0012707, shape=(), dtype=float32) 
tf.Tensor(0.99927545, shape=(), dtype=float32)
```

### regression coefficients의 sparsity 확인

<center><img  src="https://github.com/an-seunghwan/an-seunghwan.github.io/blob/master/assets/img/mcp2.png?raw=true" width="600"  height="300"></center>

처음 4

## Neural Network with MCP 
<!--stackedit_data:
eyJoaXN0b3J5IjpbNjQ3NzY1NTMzLDE0MDc3NzkwNF19
-->